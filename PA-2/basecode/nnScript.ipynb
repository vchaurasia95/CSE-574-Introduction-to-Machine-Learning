{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy.optimize import minimize\n", "from scipy.io import loadmat\n", "from math import fabs, sqrt\n", "import matplotlib.pyplot as plt\n", "import pandas as pnd\n", "import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def initializeWeights(n_in, n_out):\n", "    \"\"\"\n", "    # initializeWeights return the random weights for Neural Network given the\n", "    # number of node in the input layer and output layer\n\n", "    # Input:\n", "    # n_in: number of nodes of the input layer\n", "    # n_out: number of nodes of the output layer\n\n", "    # Output: \n", "    # W: matrix of random initial weights with size (n_out x (n_in + 1))\"\"\"\n", "    epsilon = sqrt(6) / sqrt(n_in + n_out + 1)\n", "    W = (np.random.rand(n_out, n_in + 1) * 2 * epsilon) - epsilon\n", "    return W"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigmoid(z):\n", "    \"\"\"# Notice that z can be a scalar, a vector or a matrix\n", "    # return the sigmoid of input z\"\"\"\n", "    return (1.0 / (1.0 + np.exp(-z)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def preprocess():\n", "    \"\"\" Input:\n", "     Although this function doesn't have any input, you are required to load\n", "     the MNIST data set from file 'mnist_all.mat'.\n", "     Output:\n", "     train_data: matrix of training set. Each row of train_data contains \n", "       feature vector of a image\n", "     train_label: vector of label corresponding to each image in the training\n", "       set\n", "     validation_data: matrix of training set. Each row of validation_data \n", "       contains feature vector of a image\n", "     validation_label: vector of label corresponding to each image in the \n", "       training set\n", "     test_data: matrix of training set. Each row of test_data contains \n", "       feature vector of a image\n", "     test_label: vector of label corresponding to each image in the testing\n", "       set\n", "     Some suggestions for preprocessing step:\n", "     - feature selection\"\"\"\n", "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n\n", "    # Split the training sets into two sets of 50000 randomly sampled training examples and 10000 validation examples.\n", "    # Your code here.\n", "    test = []\n", "    train = []\n", "    for i in range(10):\n", "        id_train = f'train{i}'\n", "        id_test = f'test{i}'\n", "        mat_test = mat[id_test]\n", "        mat_train = mat[id_train]\n", "        label_test = np.full((mat_test.shape[0], 1), i)\n", "        label_train = np.full((mat_train.shape[0], 1), i)\n", "        labelled_train = np.concatenate((mat_train, label_train), axis=1)\n", "        labelled_test = np.concatenate((mat_test, label_test), axis=1)\n", "        test.append(labelled_test)\n", "        train.append(labelled_train)\n", "    train_all = np.concatenate((train[0], train[1], train[2], train[3],\n", "                                train[4], train[5], train[6], train[7], train[8], train[9]), axis=0)\n", "    test_all = np.concatenate(\n", "        (test[0], test[1], test[2], test[3], test[4],\n", "         test[5], test[6], test[7], test[8], test[9]),\n", "        axis=0)\n", "    np.random.shuffle(train_all)\n", "    train_final = train_all[0:50000,:]\n", "    train_data = train_final[:, 0:784]\n", "    train_label = train_final[:, 784]\n", "    validation_final = train_all[50000:60000,:]\n", "    validation_data = validation_final[:, 0:784]\n", "    validation_label = validation_final[:, 784]\n", "    test_data = test_all[:, 0:784]\n", "    test_label = test_all[:, 784]\n", "    test_data = test_data / 255.0\n", "    validation_data = validation_data / 255.0\n", "    train_data = train_data / 255.0\n", "    # Feature selection\n", "    # Your code here.\n", "    all = np.concatenate((train_data, validation_data), axis=0)\n", "    ref = all[0, :]\n", "    redundant_vals = np.all(all == ref, axis=0)\n", "    count = 0\n", "    global selected_indicies\n", "    for i in range(len(redundant_vals)):\n", "        if redundant_vals[i] == False:\n", "            count += 1\n", "            selected_indicies.append(i)\n", "            print(i, end=\" \")\n", "    print(\" \")\n", "    print(f\"Total Selected Features-->{count}\")\n", "    final_all = all[:, ~redundant_vals]\n", "    train_row = train_data.shape[0]\n", "    train_data = final_all[0:train_row, :]\n", "    validation_data = final_all[train_row:, :]\n", "    test_data = test_data[:, ~redundant_vals]\n", "    print('preprocess done')\n", "    return train_data, train_label, validation_data, validation_label, test_data, test_label"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def nnObjFunction(params, *args):\n", "    \"\"\"% nnObjFunction computes the value of objective function (negative log \n", "    %   likelihood error function with regularization) given the parameters \n", "    %   of Neural Networks, thetraining data, their corresponding training \n", "    %   labels and lambda - regularization hyper-parameter.\n", "    % Input:\n", "    % params: vector of weights of 2 matrices w1 (weights of connections from\n", "    %     input layer to hidden layer) and w2 (weights of connections from\n", "    %     hidden layer to output layer) where all of the weights are contained\n", "    %     in a single vector.\n", "    % n_input: number of node in input layer (not include the bias node)\n", "    % n_hidden: number of node in hidden layer (not include the bias node)\n", "    % n_class: number of node in output layer (number of classes in\n", "    %     classification problem\n", "    % training_data: matrix of training data. Each row of this matrix\n", "    %     represents the feature vector of a particular image\n", "    % training_label: the vector of truth label of training images. Each entry\n", "    %     in the vector represents the truth label of its corresponding image.\n", "    % lambda: regularization hyper-parameter. This value is used for fixing the\n", "    %     overfitting problem.\n", "    % Output: \n", "    % obj_val: a scalar value representing value of error function\n", "    % obj_grad: a SINGLE vector of gradient value of error function\n", "    % NOTE: how to compute obj_grad\n", "    % Use backpropagation algorithm to compute the gradient of error function\n", "    % for each weights in weight matrices.\n", "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n", "    % reshape 'params' vector into 2 matrices of weight w1 and w2\n", "    % w1: matrix of weights of connections from input layer to hidden layers.\n", "    %     w1(i, j) represents the weight of connection from unit j in input \n", "    %     layer to unit i in hidden layer.\n", "    % w2: matrix of weights of connections from hidden layer to output layers.\n", "    %     w2(i, j) represents the weight of connection from unit j in hidden \n", "    %     layer to unit i in output layer.\"\"\"\n", "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n", "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n", "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n", "    obj_val = 0\n", "    train_rows = training_data.shape[0]\n\n", "    # Your code here\n", "    # Input to Hidden Layer\n", "    biases_1 = np.full((train_rows, 1), 1)\n", "    training_data_with_biases = np.concatenate(\n", "        (biases_1, training_data), axis=1)\n", "    bj = np.dot(training_data_with_biases, np.transpose(w1))\n", "    sigma_bj = sigmoid(bj)\n\n", "    # Hidden to Output\n", "    sigma_bj_rows = sigma_bj.shape[0]\n", "    biases_2 = np.full((sigma_bj_rows, 1), 1)\n", "    sigma_bj_with_biase = np.concatenate((biases_2, sigma_bj), axis=1)\n", "    bz = np.dot(sigma_bj_with_biase, np.transpose(w2))\n", "    sigma_bz = sigmoid(bz)\n\n", "    # Error Calculation thru Error Function\n", "    ground_truth = np.full((train_rows, n_class), 0)\n", "    for i in range(train_rows):\n", "        label = training_label[i]\n", "        ground_truth[i][label] = 1\n", "    ground_truth_prime = (1.0 - ground_truth)\n", "    sigma_bz_prime = (1.0 - sigma_bz)\n", "    lg_sigma_bz = np.log(sigma_bz)\n", "    lg_sigma_bz_prime = np.log(sigma_bz_prime)\n", "    err = np.sum(np.multiply(ground_truth, lg_sigma_bz) +\n", "                 np.multiply(ground_truth_prime, lg_sigma_bz_prime))\n", "    err = (err / ((-1)*train_rows))\n\n", "    # Gradient Calculation for BP\n", "    delta = sigma_bz - ground_truth\n", "    grad_w2 = np.dot(delta.T, sigma_bj_with_biase)\n", "    t1 = np.dot(delta,w2)\n", "    t1 = t1 * (sigma_bj_with_biase*(1.0-sigma_bj_with_biase))\n", "    grad_w1 = (np.dot(np.transpose(t1), training_data_with_biases))[1:, :]\n\n", "    # Calculate Regularization\n", "    reg_para = lambdaval * (np.sum(np.square(w1))+np.sum(np.square(w2)))/(2*train_rows)\n", "    obj_val = err + reg_para\n", "    grad_w1_reg = ((lambdaval*w1)+grad_w1)/train_rows\n", "    grad_w2_reg = ((lambdaval*w2)+grad_w2)/train_rows\n", "    obj_grad = np.concatenate((grad_w1_reg.flatten(),grad_w2_reg.flatten()),0)\n\n", "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n", "    # you would use code similar to the one below to create a flat array\n", "    # obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n", "    # obj_grad = np.array([])\n", "    return (obj_val, obj_grad)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def nnPredict(w1, w2, data):\n", "    \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n", "    % Network.\n", "    % Input:\n", "    % w1: matrix of weights of connections from input layer to hidden layers.\n", "    %     w1(i, j) represents the weight of connection from unit j in input \n", "    %     layer to unit i in hidden layer.\n", "    % w2: matrix of weights of connections from hidden layer to output layers.\n", "    %     w2(i, j) represents the weight of connection from unit j in hidden \n", "    %     layer to unit i in output layer.\n", "    % data: matrix of data. Each row of this matrix represents the feature \n", "    %       vector of a particular image\n", "    % Output: \n", "    % label: a column vector of predicted labels\"\"\"\n", "    labels = np.array([])\n", "    # Your code here\n", "    train_rows = data.shape[0]\n", "    biase1 = np.full((train_rows,1), 1)\n", "    data_wit_biases = np.concatenate((biase1,data), axis=1)\n", "    bj = sigmoid(np.dot(data_wit_biases, w1.T))\n", "    biase2 = np.full((bj.shape[0],1),1)\n", "    data2 = np.concatenate((biase2,bj),axis=1)\n", "    cj = sigmoid(np.dot(data2,w2.T))\n", "    labels = np.argmax(cj, axis=1)\n", "    return labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n**************Neural Network Script Starts here********************************\n<br>\n", "selected_indicies = []<br>\n", "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()<br>\n", "exec_time = []<br>\n", "train_acc = []<br>\n", "test_acc = []<br>\n", "validation_acc = []<br>\n", "lambdas = []<br>\n", "hidden = []<br>\n", "# set the number of nodes in input unit (not including bias unit)<br>\n", "n_input = train_data.shape[1]<br>\n", "# set the number of nodes in output unit<br>\n", "n_class = 10<br>\n", "lambada_vals = np.arange(0, 70, 10)<br>\n", "n_hidden_vals = np.arange(4, 24, 4)<br>\n", "for lambda_val in lambada_vals:<br>\n", "    for n_hidden in n_hidden_vals:<br>\n", "        start = time.time()<br>\n", "        #  Train Neural Network<br>\n", "        # initialize the weights into some random matrices<br>\n", "        initial_w1 = initializeWeights(n_input, n_hidden)<br>\n", "        initial_w2 = initializeWeights(n_hidden, n_class)<br>\n", "        # unroll 2 weight matrices into single column vector<br>\n", "        initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)<br>\n", "        args = (n_input, n_hidden, n_class, train_data, train_label, lambda_val)<br>\n", "        # Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example<br>\n", "        opts = {'maxiter': 50}  # Preferred value.<br>\n", "        nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)<br>\n", "        # In Case you want to use fmin_cg, you may have to split the nnObjectFunction to two functions nnObjFunctionVal<br>\n", "        # and nnObjGradient. Check documentation for this function before you proceed.<br>\n", "        # nn_params, cost = fmin_cg(nnObjFunctionVal, initialWeights, nnObjGradient,args = args, maxiter = 50)<br>\n", "        # Reshape nnParams from 1D vector into w1 and w2 matrices<br>\n", "        w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))<br>\n", "        w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))<br>\n", "        # Test the computed parameters<br>\n", "        predicted_label = nnPredict(w1, w2, train_data)<br>\n", "        # find the accuracy on Training Dataset<br>\n", "        training_accuracy = str(100 * np.mean((predicted_label == train_label).astype(float)))<br>\n", "        print('\\n Training set Accuracy:' + training_accuracy + '%', end=\" \")<br>\n", "        predicted_label = nnPredict(w1, w2, validation_data)<br>\n", "        validation_accuracy = str(100 * np.mean((predicted_label == validation_label).astype(float)))<br>\n", "        # find the accuracy on Validation Dataset<br>\n", "        print('\\n Validation set Accuracy:' + validation_accuracy + '%', end=\" \")<br>\n", "        predicted_label = nnPredict(w1, w2, test_data)<br>\n", "        # find the accuracy on Validation Dataset<br>\n", "        test_accuracy = str(100 * np.mean((predicted_label == test_label).astype(float)))<br>\n", "        print('\\n Test set Accuracy:' + test_accuracy + '%', end=\" \")<br>\n", "        end = time.time()<br>\n", "        exec_time.append(end - start)<br>\n", "        lambdas.append(lambda_val)<br>\n", "        hidden.append(n_hidden)<br>\n", "        test_acc.append(test_accuracy)<br>\n", "        train_acc.append(training_accuracy)<br>\n", "        validation_acc.append(validation_accuracy)<br>\n", "        print(' || n_hidden=', n_hidden, end=\" \")<br>\n", "        print(' || \u00ce\u00bb=', lambda_val)<br>\n", "# results = pnd.DataFrame(np.column_stack([lambdas, hidden, train_acc, validation_acc, test_acc, exec_time]),<br>\n", "#                        columns=['\u00ce\u00bb', 'm', 'Train_Accuracy', 'Validation_Accuracy', 'Test_Accuracy', 'Training_Time'])<br>\n", "# results = results.sort_values(by=['Test_Accuracy'], ascending=False)<br>\n", "#<br>\n", "#<br>\n", "# results.head(10)<br>\n", "#<br>\n", "# optimal_lambda = results.iloc[0,0]<br>\n", "# optimal_m = results.iloc[0,1]<br>\n", "#<br>\n", "# print(\"Optimal Lambda :\", optimal_lambda)<br>\n", "# print(\"Optimal hidden units :\", optimal_m)<br>\n", "#<br>\n", "# # In[13]:<br>\n", "#<br>\n", "#<br>\n", "# rows_with_optimal_lambda = results[results.\u00ce\u00bb == optimal_lambda]<br>\n", "# rows_with_optimal_m = results[results.m == optimal_m]<br>\n", "# rows_with_optimal_m = rows_with_optimal_m.sort_values(by=['\u00ce\u00bb'])<br>\n", "# rows_with_optimal_lambda = rows_with_optimal_lambda.sort_values(by=['m'])<br>\n", "#<br>\n", "# # Figure & Title<br>\n", "# plt.figure(figsize=(16, 12))<br>\n", "# plt.title('Accuracy vs Number of Hidden Units (m)', pad=10, fontsize=20, fontweight='bold')<br>\n", "#<br>\n", "# # Axis Labeling<br>\n", "# plt.xlabel('Number of Hidden Input (m)', labelpad=20, weight='bold', size=15)<br>\n", "# plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "#<br>\n", "# # Axis ticks<br>\n", "# plt.xticks(np.arange(4, 24, step=4), fontsize=15)<br>\n", "# plt.yticks(np.arange(70, 95, step=2), fontsize=15)<br>\n", "#<br>\n", "# plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Train_Accuracy, color='g')<br>\n", "# plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Validation_Accuracy, color='b')<br>\n", "# plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Test_Accuracy, color='r')<br>\n", "#<br>\n", "# ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "# plt.text(16, 86, s=ss, fontsize=25)<br>\n", "# plt.legend(('Training Accuracy', 'Validation Accuracy', 'Testing Accuracy'), fontsize=15)<br>\n", "# plt.show()<br>\n", "#<br>\n", "# # In[19]:<br>\n", "#<br>\n", "#<br>\n", "# # Figure & Title<br>\n", "# plt.figure(figsize=(16, 12))<br>\n", "# plt.title('Accuracy vs Number of Hidden Units (m)', pad=10, fontsize=20, fontweight='bold')<br>\n", "#<br>\n", "# # Axis Labeling<br>\n", "# plt.xlabel('Number of Hidden Input (m)', labelpad=20, weight='bold', size=15)<br>\n", "# plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "#<br>\n", "# # Axis ticks<br>\n", "# plt.xticks(np.arange(4, 24, step=4), fontsize=15)<br>\n", "# plt.yticks(np.arange(70, 95, step=2), fontsize=15)<br>\n", "#<br>\n", "# plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Train_Accuracy, color='g')<br>\n", "# plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Validation_Accuracy, color='b')<br>\n", "# plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Test_Accuracy, color='r')<br>\n", "#<br>\n", "# ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "# plt.text(16, 86, s=ss, fontsize=25)<br>\n", "# plt.legend(('Training Accuracy', 'Validation Accuracy', 'Testing Accuracy'), fontsize=15)<br>\n", "# plt.show()<br>\n", "#<br>\n", "# # ## <font color=blue> Training Time vs Number of Hidden Units<br>\n", "#<br>\n", "# # In[28]:<br>\n", "#<br>\n", "#<br>\n", "# # Figure & Title<br>\n", "# plt.figure(figsize=(16, 12))<br>\n", "# plt.title('Training_Time vs Number of Hidden Units(m)', pad=10, fontsize=20, fontweight='bold')<br>\n", "#<br>\n", "# # Axis Labeling<br>\n", "# plt.xlabel('Number of Hidden Input', labelpad=20, weight='bold', size=15)<br>\n", "# plt.ylabel('Training_Time', labelpad=20, weight='bold', size=15)<br>\n", "#<br>\n", "# # Axis ticks<br>\n", "# plt.xticks(np.arange(4, 24, step=4), fontsize=15)<br>\n", "# plt.yticks(fontsize=15)<br>\n", "#<br>\n", "# ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "# plt.text(8, 24.25, s=ss, fontsize=25)<br>\n", "# plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Training_Time, color='c')<br>\n", "#<br>\n", "# plt.show()<br>\n", "#<br>\n", "# # ## <font color=blue> Accuracy vs Lamda<br>\n", "#<br>\n", "# # In[26]:<br>\n", "#<br>\n", "#<br>\n", "# # Figure & Title<br>\n", "# plt.figure(figsize=(16, 12))<br>\n", "# plt.title('Accuracy vs \u00ce\u00bb', pad=10, fontsize=20, fontweight='bold')<br>\n", "#<br>\n", "# # Axis Labeling<br>\n", "# plt.xlabel('\u00ce\u00bb', labelpad=20, weight='bold', size=15)<br>\n", "# plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "#<br>\n", "# # Axis ticks<br>\n", "# plt.xticks(np.arange(0, 65, step=5), fontsize=15)<br>\n", "# plt.yticks(fontsize=15)<br>\n", "#<br>\n", "# plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Train_Accuracy, color='g')<br>\n", "# plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Validation_Accuracy, color='b')<br>\n", "# plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Test_Accuracy, color='r')<br>\n", "#<br>\n", "# ss = 'm = ' + str(optimal_m) + ''<br>\n", "# plt.text(10, 93.5, s=ss, fontsize=25)<br>\n", "# plt.legend(('Training Accuracy', 'Validation Accuracy', 'Testing Accuracy'), fontsize=15)<br>\n", "# plt.show()<br>\n", "#<br>\n", "# # In[22]:<br>\n", "#<br>\n", "#<br>\n", "# len(selected_indicies)<br>\n", "#<br>\n", "# # # <font color = green> Pickle object Creation with Optimal parameters<br>\n", "#<br>\n", "# # In[29]:<br>\n", "#<br>\n", "#<br>\n", "# # set the number of nodes in input unit (not including bias unit)<br>\n", "# n_input = train_data.shape[1]<br>\n", "#<br>\n", "# # set the number of nodes in hidden unit (not including bias unit)<br>\n", "# n_hidden = 20<br>\n", "#<br>\n", "# # set the number of nodes in output unit<br>\n", "# n_class = 10<br>\n", "#<br>\n", "# # initialize the weights into some random matrices<br>\n", "# initial_w1 = initializeWeights(n_input, n_hidden)<br>\n", "# initial_w2 = initializeWeights(n_hidden, n_class)<br>\n", "#<br>\n", "# # unroll 2 weight matrices into single column vector<br>\n", "# initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)<br>\n", "#<br>\n", "# # set the regularization hyper-parameter<br>\n", "# lambdaval = 30<br>\n", "#<br>\n", "# args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)<br>\n", "#<br>\n", "# # Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example<br>\n", "#<br>\n", "# opts = {'maxiter': 50}  # Preferred value.<br>\n", "#<br>\n", "# nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)<br>\n", "#<br>\n", "# # In Case you want to use fmin_cg, you may have to split the nnObjectFunction to two functions nnObjFunctionVal<br>\n", "# # and nnObjGradient. Check documentation for this function before you proceed.<br>\n", "# # nn_params, cost = fmin_cg(nnObjFunctionVal, initialWeights, nnObjGradient,args = args, maxiter = 50)<br>\n", "#<br>\n", "#<br>\n", "# # Reshape nnParams from 1D vector into w1 and w2 matrices<br>\n", "# w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))<br>\n", "# w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))<br>\n", "#<br>\n", "# # Test the computed parameters<br>\n", "#<br>\n", "# predicted_label = nnPredict(w1, w2, train_data)<br>\n", "#<br>\n", "# # find the accuracy on Training Dataset<br>\n", "#<br>\n", "# print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')<br>\n", "#<br>\n", "# predicted_label = nnPredict(w1, w2, validation_data)<br>\n", "#<br>\n", "# # find the accuracy on Validation Dataset<br>\n", "#<br>\n", "# print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')<br>\n", "#<br>\n", "# predicted_label = nnPredict(w1, w2, test_data)<br>\n", "#<br>\n", "# # find the accuracy on Validation Dataset<br>\n", "#<br>\n", "# print('\\n Test set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')<br>\n", "#<br>\n", "# parameters = [selected_indicies, int(optimal_m), w1, w2, int(optimal_lambda)]<br>\n", "# pickle.dump(parameters, open('params.pickle', 'wb'))<br>\n", "results = pnd.DataFrame(np.column_stack([lambdas, hidden, train_acc, validation_acc, test_accuracy, exec_time]),<br>\n", "                      columns=['\u00ce\u00bb', 'm','Train_Accuracy', 'Validation_Accuracy', 'Test_Accuracy', 'Training_Time'])<br>\n", "results = results.sort_values(by=['Test_Accuracy'], ascending=False)<br>\n", "# In[11]:<br>\n", "results.head(10)<br>\n", "# In[12]:<br>\n", "optimal_lambda = results.iloc[0,0]<br>\n", "optimal_m = results.iloc[0,1]<br>\n", "print(\"Optimal Lambda :\",optimal_lambda)<br>\n", "print(\"Optimal hidden units :\", optimal_m)<br>\n", "# In[13]:<br>\n", "rows_with_optimal_lambda = results[results.\u00ce\u00bb == optimal_lambda]<br>\n", "rows_with_optimal_m      = results[results.m == optimal_m]<br>\n", "rows_with_optimal_m<br>\n", "rows_with_optimal_m = rows_with_optimal_m.sort_values(by=['\u00ce\u00bb'])<br>\n", "rows_with_optimal_m<br>\n", "rows_with_optimal_lambda<br>\n", "rows_with_optimal_lambda = rows_with_optimal_lambda.sort_values(by=['m'])<br>\n", "rows_with_optimal_lambda<br>\n", "# Figure & Title<br>\n", "plt.figure(figsize=(16,12))<br>\n", "plt.title('Accuracy vs Number of Hidden Units (m)', pad=10, fontsize = 20, fontweight = 'bold')<br>\n", "# Axis Labeling<br>\n", "plt.xlabel('Number of Hidden Input (m)',labelpad=20, weight='bold', size=15)<br>\n", "plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "# Axis ticks<br>\n", "plt.xticks( np.arange( 4,24, step=4), fontsize = 15)<br>\n", "plt.yticks( np.arange(70,95, step=2), fontsize = 15)<br>\n", "plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Train_Accuracy,  color='g')<br>\n", "plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Validation_Accuracy, color='b')<br>\n", "plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Test_Accuracy,  color='r')<br>\n", "ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "plt.text(16,86, s=ss, fontsize=25)<br>\n", "plt.legend(('Training Accuracy','Validation Accuracy','Testing Accuracy'),fontsize = 15)<br>\n", "plt.show()<br>\n", "# In[19]:<br>\n", "# Figure & Title<br>\n", "plt.figure(figsize=(16,12))<br>\n", "plt.title('Accuracy vs Number of Hidden Units (m)', pad=10, fontsize = 20, fontweight = 'bold')<br>\n", "# Axis Labeling<br>\n", "plt.xlabel('Number of Hidden Input (m)',labelpad=20, weight='bold', size=15)<br>\n", "plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "# Axis ticks<br>\n", "plt.xticks( np.arange( 4,24, step=4), fontsize = 15)<br>\n", "plt.yticks( np.arange(70,95, step=2), fontsize = 15)<br>\n", "plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Train_Accuracy,  color='g')<br>\n", "plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Validation_Accuracy, color='b')<br>\n", "plt.scatter(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Test_Accuracy,  color='r')<br>\n", "ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "plt.text(16,86, s=ss, fontsize=25)<br>\n", "plt.legend(('Training Accuracy','Validation Accuracy','Testing Accuracy'),fontsize = 15)<br>\n", "plt.show()<br>\n", "# ## <font color=blue> Training Time vs Number of Hidden Units<br>\n", "# In[28]:<br>\n", "# Figure & Title<br>\n", "plt.figure(figsize=(16,12))<br>\n", "plt.title('Training_Time vs Number of Hidden Units(m)', pad=10, fontsize = 20, fontweight = 'bold')<br>\n", "# Axis Labeling<br>\n", "plt.xlabel('Number of Hidden Input',labelpad=20, weight='bold', size=15)<br>\n", "plt.ylabel('Training_Time', labelpad=20, weight='bold', size=15)<br>\n", "# Axis ticks<br>\n", "plt.xticks( np.arange( 4,24, step=4), fontsize = 15)<br>\n", "plt.yticks( fontsize = 15)<br>\n", "ss = '\u00ce\u00bb = ' + str(optimal_lambda) + ''<br>\n", "plt.text(8,24.25, s=ss, fontsize=25)<br>\n", "plt.plot(rows_with_optimal_lambda.m, rows_with_optimal_lambda.Training_Time,  color='c')<br>\n", "plt.show()<br>\n", "# ## <font color=blue> Accuracy vs Lamda<br>\n", "# In[26]:<br>\n", "# Figure & Title<br>\n", "plt.figure(figsize=(16,12))<br>\n", "plt.title('Accuracy vs \u00ce\u00bb', pad=10, fontsize = 20, fontweight = 'bold')<br>\n", "# Axis Labeling<br>\n", "plt.xlabel('\u00ce\u00bb'        ,labelpad=20, weight='bold', size=15)<br>\n", "plt.ylabel('Accuracy', labelpad=20, weight='bold', size=15)<br>\n", "# Axis ticks<br>\n", "plt.xticks( np.arange( 0,65, step=5), fontsize = 15)<br>\n", "plt.yticks( fontsize = 15)<br>\n", "plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Train_Accuracy,  color='g')<br>\n", "plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Validation_Accuracy, color='b')<br>\n", "plt.plot(rows_with_optimal_m.\u00ce\u00bb, rows_with_optimal_m.Test_Accuracy,  color='r')<br>\n", "ss = 'm = ' + str(optimal_m) + ''<br>\n", "plt.text(10,93.5, s=ss, fontsize=25)<br>\n", "plt.legend(('Training Accuracy','Validation Accuracy','Testing Accuracy'),fontsize = 15)<br>\n", "plt.show()<br>\n", "# In[22]:<br>\n", "len(selected_indicies)<br>\n", "# # <font color = green> Pickle object Creation with Optimal parameters<br>\n", "# In[29]:<br>\n", "# set the number of nodes in input unit (not including bias unit)<br>\n", "n_input = train_data.shape[1]<br>\n", "# set the number of nodes in hidden unit (not including bias unit)<br>\n", "n_hidden = 20<br>\n", "# set the number of nodes in output unit<br>\n", "n_class = 10<br>\n", "# initialize the weights into some random matrices<br>\n", "initial_w1 = initializeWeights(n_input, n_hidden)<br>\n", "initial_w2 = initializeWeights(n_hidden, n_class)<br>\n", "# unroll 2 weight matrices into single column vector<br>\n", "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)<br>\n", "# set the regularization hyper-parameter<br>\n", "lambdaval = 30<br>\n", "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)<br>\n", "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example<br>\n", "opts = {'maxiter': 50}  # Preferred value.<br>\n", "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)<br>\n", "# In Case you want to use fmin_cg, you may have to split the nnObjectFunction to two functions nnObjFunctionVal<br>\n", "# and nnObjGradient. Check documentation for this function before you proceed.<br>\n", "# nn_params, cost = fmin_cg(nnObjFunctionVal, initialWeights, nnObjGradient,args = args, maxiter = 50)<br>\n", "# Reshape nnParams from 1D vector into w1 and w2 matrices<br>\n", "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))<br>\n", "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))<br>\n", "# Test the computed parameters<br>\n", "predicted_label = nnPredict(w1, w2, train_data)<br>\n", "# find the accuracy on Training Dataset<br>\n", "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')<br>\n", "predicted_label = nnPredict(w1, w2, validation_data)<br>\n", "# find the accuracy on Validation Dataset<br>\n", "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')<br>\n", "predicted_label = nnPredict(w1, w2, test_data)<br>\n", "# find the accuracy on Validation Dataset<br>\n", "print('\\n Test set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')<br>\n", "parameters = [selected_indicies, int(optimal_m), w1, w2, int(optimal_lambda)]<br>\n", "pickle.dump(parameters, open('params.pickle', 'wb'))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}